{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEc6eYErhGIC"
   },
   "source": [
    "## SENTIMENT ANALYSIS PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFqPVbmDhfBJ"
   },
   "source": [
    "This project's workflow and objectives are as follows:\n",
    "\n",
    "Create a \"Text==> Vader Pipeline\"\n",
    "\n",
    "Cretae a \"Text ==> Remove Stopwords ==> Vader Pipeline\"\n",
    "\n",
    "Create a \"Text ==> Remove Stopwords ==>  Bag Of Words ==> Custom Model\"\n",
    "\n",
    "Create a \"Text ==> Remove Stopwords ==>  TF-IDF ==> Custom Model\"\n",
    "\n",
    "Project scope\n",
    "\n",
    "Data Collection\n",
    "\n",
    "Text Processing\n",
    "\n",
    "Modelling\n",
    "\n",
    "Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9EQdL3b9Dik8"
   },
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9Mwkl7ahx4O",
    "outputId": "91167f74-9d1b-4491-86b2-8dcbbc1a3bec"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\t_ongep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\t_ongep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))   #breaks text into manageable pieces (tokens)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "#stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "sou-BKYXadtf",
    "outputId": "c96f993b-3208-48fa-83e5-e02e891d7be8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>an absolute masterpiece: I am quite sure any o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Buyer beware: This is a self-published book, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Glorious story: I loved Whisper of the wicked ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>A FIVE STAR BOOK: I just finished reading Whis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Whispers of the Wicked Saints: This was a easy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels                                               text\n",
       "0  __label__2  Stuning even for the non-gamer: This sound tra...\n",
       "1  __label__2  The best soundtrack ever to anything.: I'm rea...\n",
       "2  __label__2  Amazing!: This soundtrack is my favorite music...\n",
       "3  __label__2  Excellent Soundtrack: I truly like this soundt...\n",
       "4  __label__2  Remember, Pull Your Jaw Off The Floor After He...\n",
       "5  __label__2  an absolute masterpiece: I am quite sure any o...\n",
       "6  __label__1  Buyer beware: This is a self-published book, a...\n",
       "7  __label__2  Glorious story: I loved Whisper of the wicked ...\n",
       "8  __label__2  A FIVE STAR BOOK: I just finished reading Whis...\n",
       "9  __label__2  Whispers of the Wicked Saints: This was a easy..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load dataset\n",
    "train_data = pd.read_csv(\"e commerce reviews train.csv\")\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJAHTbH69vqa",
    "outputId": "ab5789dd-282a-46bc-9e27-9af722638c15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buyer beware: This is a self-published book, and if you want to know why--read a few paragraphs! Those 5 star reviews must have been written by Ms. Haddon's family and friends--or perhaps, by herself! I can't imagine anyone reading the whole thing--I spent an evening with the book and a friend and we were in hysterics reading bits and pieces of it to one another. It is most definitely bad enough to be entered into some kind of a \"worst book\" contest. I can't believe Amazon even sells this kind of thing. Maybe I can offer them my 8th grade term paper on \"To Kill a Mockingbird\"--a book I am quite sure Ms. Haddon never heard of. Anyway, unless you are in a mood to send a book to someone as a joke---stay far, far away from this one!\n"
     ]
    }
   ],
   "source": [
    "print(train_data.iloc[6][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "mEezOP_O1UgK",
    "outputId": "7cddf91d-aa3f-42d1-ac58-6bb50ec2394d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Great CD: My lovely Pat has one of the GREAT v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>One of the best game music soundtracks - for a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>__label__1</td>\n",
       "      <td>Batteries died within a year ...: I bought thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>works fine, but Maha Energy is better: Check o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>__label__2</td>\n",
       "      <td>Great for the non-audiophile: Reviewed quite a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       labels                                               text\n",
       "0  __label__2  Great CD: My lovely Pat has one of the GREAT v...\n",
       "1  __label__2  One of the best game music soundtracks - for a...\n",
       "2  __label__1  Batteries died within a year ...: I bought thi...\n",
       "3  __label__2  works fine, but Maha Energy is better: Check o...\n",
       "4  __label__2  Great for the non-audiophile: Reviewed quite a..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv(\"e commerce reviews test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN5s1CAs-7x1",
    "outputId": "af66bfda-3d4e-467d-80e4-6f2dd72d57eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     labels                                               text\n",
      "0  positive  Stuning even for the non-gamer: This sound tra...\n",
      "1  positive  The best soundtrack ever to anything.: I'm rea...\n",
      "2  positive  Amazing!: This soundtrack is my favorite music...\n",
      "3  positive  Excellent Soundtrack: I truly like this soundt...\n",
      "4  positive  Remember, Pull Your Jaw Off The Floor After He...\n",
      "     labels                                               text\n",
      "0  positive  Great CD: My lovely Pat has one of the GREAT v...\n",
      "1  positive  One of the best game music soundtracks - for a...\n",
      "2  negative  Batteries died within a year ...: I bought thi...\n",
      "3  positive  works fine, but Maha Energy is better: Check o...\n",
      "4  positive  Great for the non-audiophile: Reviewed quite a...\n"
     ]
    }
   ],
   "source": [
    "#Change the label names\n",
    "mapping_values = {\n",
    "  \"__label__1\"\t:\"negative\",\n",
    "  \"__label__2\" : \"positive\"\n",
    "\n",
    "}\n",
    "train_data[\"labels\"] = train_data[\"labels\"].map(mapping_values)\n",
    "test_data[\"labels\"] = test_data[\"labels\"].map(mapping_values)\n",
    "print(train_data.head())\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83YRjSWrhGKH"
   },
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2-kP1wurhv1h"
   },
   "outputs": [],
   "source": [
    "# Function to remove stopwords\n",
    "def remove_stopwords(text, stop_words=stop_words):\n",
    "    \"\"\"\n",
    "    Purpose: This function removes stopwords from a given text string to enhance the quality of text data for natural language processing tasks.\n",
    "\n",
    "    Parameters:\n",
    "    text: The input text from which stopwords will be removed.\n",
    "    stop_words: A predefined list of stopwords (default is a list called stop_words).\n",
    "\n",
    "    Process:\n",
    "    Tokenization: Splits the input text into individual words using NLTK's word_tokenize function.\n",
    "    Filtering: Creates a new list of words that excludes any words found in the stop_words list, ignoring case.\n",
    "    Reconstruction: Joins the filtered words back into a single string, effectively reconstructing the text without stopwords.\n",
    "\n",
    "    Return Value: The function returns the filtered text, which can improve the subsequent analysis and processing of text data by focusing on meaningful words rather than common, less informative ones.\n",
    "    \"\"\"\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Remove stopwords from the text\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Reconstruct the text without stopwords\n",
    "    filtered_text = \" \".join(filtered_words)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "id": "KcXmLBC23uwS",
    "outputId": "43d05952-137d-4daa-9646-f8ecb2dcd14a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Stuning even non-gamer : sound track beautiful...\n",
       "1    best soundtrack ever anything . : 'm reading l...\n",
       "2    Amazing ! : soundtrack favorite music time , h...\n",
       "3    Excellent Soundtrack : truly like soundtrack e...\n",
       "4    Remember , Pull Jaw Floor Hearing : 've played...\n",
       "5    absolute masterpiece : quite sure actually tak...\n",
       "6    Buyer beware : self-published book , want know...\n",
       "7    Glorious story : loved Whisper wicked saints ....\n",
       "8    FIVE STAR BOOK : finished reading Whisper Wick...\n",
       "9    Whispers Wicked Saints : easy read book made w...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"text\"].head(10).apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-gBkyhP-aiX",
    "outputId": "3b45f2ae-06de-4670-a0b1-2eec28d6e429"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7f72339c9f42cab4face979e40fbae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3600010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm   #The tqdm library is used to display a progress bar for iterations in Python code, making it easier to monitor the progress of operations, especially for long-running processes.\n",
    "\n",
    "\n",
    "\n",
    "tqdm.pandas()\n",
    "train_data['stop words'] = train_data['text'].progress_apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "odxSN-jFMf_7"
   },
   "outputs": [],
   "source": [
    "#save the processed text data to my local machine\n",
    "train_data.to_csv(r\"C:\\Users\\t_ongep\\Downloads\\processed_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6zg2cKT_utE",
    "outputId": "8fc98bd0-55aa-4161-dd3e-ec05d3f0a3ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e56e9f9be9724baa8a570c0c639c3447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tqdm.pandas()\n",
    "test_data[\"stop words\"] = test_data[\"text\"].progress_apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFneH-Jc_Urn"
   },
   "source": [
    "## Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OXtWxeVO17Qh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zVfpmaQ6uCoT"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer   # creates a \"bag-of-words\" representation by counting the number of times each word appears in the documents.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer        # adds another layer of complexity by not only counting how often a word appears in a document (term frequency) but also taking into account how rare or common the word is across all documents (inverse document frequency).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "V4eqAPTX2Dfv"
   },
   "outputs": [],
   "source": [
    "#create a bag of words(BoW) from stop words column\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(train_data[\"stop words\"])\n",
    "X_test_bow = vectorizer.transform(test_data[\"stop words\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "VufjO4US9qIR"
   },
   "outputs": [],
   "source": [
    "#creating a tfidf matrix from stop words colum\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_data[\"stop words\"])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data[\"stop words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "K_5tBjcQ_q78",
    "outputId": "7825bacc-7fce-4238-d146-30d6c76bb3b1"
   },
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7C69GOfATD3"
   },
   "source": [
    "### Modelling and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#steps\n",
    "# vADER on normal sentence\n",
    "#VADER on sentences without stopwords\n",
    "#custom on train_tfidf\n",
    "#custom on train_bow\n",
    "#Identify the model with the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kpiRliPAAa5o",
    "outputId": "f909a8ca-4823-46b6-9ed6-86287086bd54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\t_ongep\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#vader on normal sentence\n",
    "#Vader is a pretrained model for analyzing sentiment in sentences\n",
    "import nltk\n",
    "\n",
    "# download the VADER lexicon and model\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "#import the SentimentIntensityAnalyzer class from vader, this class is where the magic happens\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "rhBV9rLMA7EO"
   },
   "outputs": [],
   "source": [
    "#sentiment analyzer function to get polarity scores, then we apply the function\n",
    "\n",
    "def analyze_sentence(sentence, threshold=0):\n",
    "    \"\"\"\n",
    "    Analyze the sentiment of a given sentence and classify it as positive or negative.\n",
    "\n",
    "    # Parameters:\n",
    "    - sentence (str): The input sentence whose sentiment is to be analyzed.\n",
    "    - threshold (float): The cutoff score to determine sentiment classification. \n",
    "                         Sentiment is classified as \"positive\" if the compound score \n",
    "                         is greater than this threshold; otherwise, it is classified as \"negative\".\n",
    "                         \n",
    "    # Returns:\n",
    "    - str: The sentiment classification of the input sentence (\"positive\" or \"negative\").\n",
    "    \n",
    "    # Example:\n",
    "    >>> analyze_sentence(\"I love this product!\", threshold=0.1)\n",
    "    'positive'\n",
    "    \n",
    "    >>> analyze_sentence(\"This is the worst experience I've ever had.\", threshold=0.1)\n",
    "    'negative'\n",
    "    \"\"\"\n",
    "    \n",
    "    sentiment_scores = analyzer.polarity_scores(sentence)\n",
    "    compound_score = sentiment_scores['compound']\n",
    "\n",
    "    if compound_score > threshold:\n",
    "        sentiment = \"positive\"\n",
    "    else:\n",
    "        sentiment = \"negative\"\n",
    "\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LegurwFuBDRy",
    "outputId": "cb8e33b7-da90-4913-deb1-9a8df87404f5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a686462736a040f58765919b9111f5d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "tqdm.pandas()\n",
    "vader_on_text = test_data['text'].progress_apply(analyze_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYjPmhOnBi_G"
   },
   "source": [
    "USING THE ACCURACY METRICS AND CLASSIFIACTION REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PmGVK8CGBkfL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eX89YfIHB070",
    "outputId": "c50deb92-691e-4376-d3a9-663dbb1c5d4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.716675"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(vader_on_text, test_data['labels']) #vader_on_text model scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZaKNBuA-CT3j",
    "outputId": "9de5492c-3ca6-47b8-e0a0-6a61d07b5d4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.51      0.64    200000\n",
      "    positive       0.65      0.92      0.76    200000\n",
      "\n",
      "    accuracy                           0.72    400000\n",
      "   macro avg       0.76      0.72      0.70    400000\n",
      "weighted avg       0.76      0.72      0.70    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data['labels'],vader_on_text ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2GOXBU9znTPV",
    "outputId": "526c4d9f-e1fb-467e-c4b9-be2be596bdda"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28249a904b924b7a9f96a70b60b0e0a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#VADER ON STOPWORDS\n",
    "#now lets repeat on stopwords, lets see if by removing context irrelvant words we can improve the scores of vader\n",
    "vader_on_stopwords = test_data['stop words'].progress_apply(analyze_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_wTeAZDininl",
    "outputId": "d32015d7-ed6d-45cf-d77f-501bc704c28e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.68083"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we get the accuracy scores, then the classifcation report\n",
    "accuracy_score(vader_on_stopwords, test_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7iuLdbwHn-k9",
    "outputId": "7dd13933-aa40-45ac-b3a2-e4e1f3e13b45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.43      0.57    200000\n",
      "    positive       0.62      0.93      0.75    200000\n",
      "\n",
      "    accuracy                           0.68    400000\n",
      "   macro avg       0.74      0.68      0.66    400000\n",
      "weighted avg       0.74      0.68      0.66    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_data['labels'],vader_on_stopwords ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laDzwW8yoKLD",
    "outputId": "82846ca8-a88d-4380-934d-8d773a813734"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.86      0.85    200000\n",
      "    positive       0.85      0.84      0.85    200000\n",
      "\n",
      "    accuracy                           0.85    400000\n",
      "   macro avg       0.85      0.85      0.85    400000\n",
      "weighted avg       0.85      0.85      0.85    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###TRAINING AND TESTING CUSTOM MODELS: Multinomial NB\n",
    "##choosing it for its simplicity, speed and compatibility with bag of words and tfidf\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#create a classifier\n",
    "classifier = MultinomialNB()\n",
    "#fit on bag_of_words\n",
    "classifier.fit(X_train_bow, train_data['labels'])\n",
    "\n",
    "##lets make predictions and evaluate the model\n",
    "\n",
    "y_pred = classifier.predict(X_test_bow)\n",
    "accuracy = accuracy_score(test_data['labels'], y_pred)\n",
    "\n",
    "#printing results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(test_data['labels'], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZI75aCslqNQ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.84      0.83    200000\n",
      "    positive       0.84      0.82      0.83    200000\n",
      "\n",
      "    accuracy                           0.83    400000\n",
      "   macro avg       0.83      0.83      0.83    400000\n",
      "weighted avg       0.83      0.83      0.83    400000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create and train a second classifier on tf-idf\n",
    "classifier2 = MultinomialNB()\n",
    "classifier2.fit(X_train_tfidf, train_data[\"labels\"])\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(test_data['labels'], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(test_data['labels'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfPGBprIulfI"
   },
   "source": [
    "**CONCLUSION**\n",
    "The best performing model/pipeline is text to stopword remover o bag of words to MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Cl6o53nFyYUD"
   },
   "outputs": [],
   "source": [
    "def inference(text):\n",
    "  filtered_text = remove_stopwords(text)\n",
    "  bow = vectorizer.transform([filtered_text])\n",
    "  sentiment = classifier.predict(bow)\n",
    "  return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already present\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Load your models and vectorizer\n",
    "vectorizer = joblib.load('vectorizer.pkl')               # Make sure you have this model saved\n",
    "classifier = joblib.load('classifier.pkl')               # Your trained MultinomialNB model\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "@app.route('/inference', methods=['POST'])  # Use POST for better practice\n",
    "def inference():\n",
    "    data = request.json\n",
    "    text = data.get('text')\n",
    "\n",
    "    if text:\n",
    "        filtered_text = remove_stopwords(text)\n",
    "        bow = vectorizer.transform([filtered_text])\n",
    "        sentiment = classifier.predict(bow)\n",
    "        return jsonify({\"sentiment\": sentiment[0]})  # Return sentiment as JSON\n",
    "    else:\n",
    "        return jsonify({\"error\": \"No text provided\"}), 400  # Bad request if no text is given\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uk-0I8_JxIMF"
   },
   "source": [
    "SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Q98PJhnZxMa2"
   },
   "outputs": [],
   "source": [
    "##create an inference function to recive a text, remove stopwords, convert to bow and pass to MUltinomialNB model\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text, stop_words=stop_words):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    # Remove stopwords from the text\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    # Reconstruct the text without stopwords\n",
    "    filtered_text = \" \".join(filtered_words)\n",
    "    \n",
    "    return filtered_text\n",
    "\n",
    "def inference(text):\n",
    "    filtered_text = remove_stopwords(text)\n",
    "    bow = vectorizer.transform([filtered_text])\n",
    "    sentiment = classifier.predict(bow)\n",
    "    \n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OckEaXMtzV3f",
    "outputId": "4b79e471-3ab0-4ec7-d00b-dcf6ce0cb931"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype='<U8')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"i hate this book.\"\n",
    "inference(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2r8h_4XGC7B"
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model and vectorizer\n",
    "joblib.dump(vectorizer, 'vectorizer.pkl')\n",
    "joblib.dump(classifier, 'classifier.pkl')\n",
    "\n",
    "# Load the model and vectorizer\n",
    "vectorizer = joblib.load('vectorizer.pkl')\n",
    "classifier = joblib.load('classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
